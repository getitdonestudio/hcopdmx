\documentclass{scrartcl}

\usepackage{_styles/memoCK}
\setmainlanguage{german}
\setotherlanguages{english}
\addbibresource{references.bib}


\title{Plancks Hexadekachor}
\author{Christian Kassung \and Jürgen P.\ Rabe \and Matthias Staudacher \and José D.\ Cojal González}
\date{\today}


\begin{document}

\maketitle

\setcounter{tocdepth}{1}


\newpage
\tableofcontents

\newpage \addsec{Newtonsche Mechanik $(0,0,0,0)$ CK $\rightarrow$ ok}
\label{sec:0000}

Man stelle sich eine Welt vor, in der die Schwerkraft vernachlässigbar ist. In der sich Dinge im Vergleich zur Lichtgeschwindigkeit sehr langsam bewegen. In der Quanteneffekte zu klein sind, um wahrgenommen werden zu können. Und in der die Anzahl der Teilchen sehr begrenzt ist. Eine solche Welt wäre vollständig durch Galileis Raum-Zeitmodell und die Newtonsche Mechanik beschrieben.

Im Hexadekachor-Modell entspricht dies der Theorie, in der die Gravitationskonstante $G$, der Kehrwert der Lichtgeschwindigkeit $c^{–1}$, das Plancksche Wirkungsquantum $h$ und die Boltzmann-Konstante $\kboltz$ allesamt verschwinden: Dies ist das kleine Tetraeder im Zentrum des Modells. Dieser symbolisiert den Ursprung der modernen Physik sowohl in konzeptioneller als auch in historischer Hinsicht. Alle Gegenstände in dieser Welt haben einen wohldefinierten Ort im absoluten Raum. Die Zeit ist ein Parameter, der überall gleich und unentwegt das Verrinnen von Sekunden, Minuten und Stunden zählt. Zu jeder Bewegung gehört ein fester, berechenbarer Weg. Und jede Beziehung zwischen den Teilchen ist festgelegt. Klingt das vertraut? Wie wir alle wissen, beschreibt diese Theorie nicht die Realität. Sie ist lediglich eine erste, aber kontrollierte Annäherung an die Vielfalt der physischen Welt um uns herum.


\subsection*{Newtons Grundlagen}

Aus der Sicht der modernen Physik beschäftigt sich die Newtonsche Mechanik mit der Bewegung hinreichend langsamer klassischer Körper unter dem Einfluss eines Systems beliebiger physikalischer Kräfte. Daher kennt die reine Newtonsche Mechanik \textit{per se} keine Schwerkraft, kein Licht, kein Wirkungsquantum, und es ist ihr egal, ob es nur wenige oder sehr viele Teilchen gibt. Sie ist die Theorie $(0,0,0,0)$ des Hexadekachor-Modells und definiert den Ursprung eines kartesischen Koordinatensystems, dessen Achsen durch vier grundlegende Naturkonstanten definiert sind. Gleichzeitig weist dieses System allen anderen Theorien ihren richtigen Platz in diesem Modell zu.

Newton fasste seine Erkenntnisse in drei Gesetzen zusammen. Das zweite Gesetz ist wohl das wichtigste und besagt, dass die auf einen Körper ausgeübte Kraft $\vec{F}$ gleich der Masse $m$ dieses Körpers mal seiner Beschleunigung $\vec{a}$ ist:

\begin{equation*}
  \vec{F} =m\, \vec{a}\,.
\end{equation*}
%
In dieser Formel treten eindeutig keine fundamentalen Konstanten auf.

Wenn man die Gravitation berücksichtigt, bleibt die Newtonsche Mechanik intakt. Sie wird lediglich zur Theorie $(G,0,0,0)$ erweitert, indem man Newtons Gravitationsgesetz und die grundlegende Idee der Identität von träger und schwerer Masse hinzufügt. Damit definiert diese Theorie ein neues Tetraeder des Modells.


\subsection*{Galileis Raum-Zeitmodell}

Die Newtonsche Mechanik basiert auf Galileis Modell von Raum und Zeit. Sie geht davon aus, dass zwei Inertialsysteme physikalisch ununterscheidbar sind. In einem berühmten Gedankenexperiment hat Galilei ein Schiff betrachtet, das sich geradlinig mit konstanter Geschwindigkeit bewegt. Unter Deck, alle Luken dicht und kein Mobiltelefon zur Hand, kann man zwar die eigene Bewegung bezogen auf das Schiff bestimmen, nicht aber bezogen auf die See. Dieses Gedankenexperiment war für Galilei von solcher Strahlkraft, dass er es im Frontispiz seines \enquote{Dialogs über die zwei Weltsysteme} prominent zwischen den Figuren von Aristoteles, Ptolemäus und Kopernikus verewigte. Darüber hinaus macht die Newtonsche Mechanik eine absolute Zeit geltend, die in sämtlichen Inertialsystemen gleich verläuft. Die Geschwindigkeiten von Inertialsystemen addieren sich linear. Damit ist es möglich, alltägliche mechanische Systeme hinreichend genau zu beschreiben.


\subsection*{Nachträgliche Korrekturen}

Galileis Modell von Raum und Zeit führt jedoch zu logischen Widersprüchen mit den Maxwell-Gleichungen der Elektrodynamik. Daher sind für große Geschwindigkeiten grundlegende Korrekturen notwendig, die nur im Rahmen von Einsteins spezieller Relativitätstheorie verständlich sind, also in der Theorie $(0,c^{-1},0,0)$.

Noch drastischere Korrekturen erfährt die klassische Mechanik auf atomaren und subatomaren Längenskalen. Sie verliert ihren deterministischen Charakter und wurde im ersten Drittel des 20.\ Jahrhunderts im Rahmen der Quantenmechanik $(0,0,h,0)$ als Theorie der Wahrscheinlichkeitsamplituden neu formuliert.

Schon vorher hatten Maxwell und Boltzmann und viele andere gezeigt, dass die Thermodynamik als statistische Theorie sehr vieler klassischer, Newtonscher Teilchen verstanden werden kann, als klassische statistische Mechanik $(0,0,0,\kboltz)$.

Sämtliche Korrekturen zur klassischen Mechanik verweisen also auf neue Tetraeder des Modells. Die beiden Theorien $(0,0,0,0)$, die Newtonsche Mechanik, und $(G,0,0,0)$, die Newtonsche Gravitationstheorie, wurden von Newton zeitgleich entwickelt; hier widerspricht also die Systematik des Hexadekachors in gewisser Weise der Geschichte der Physik. Newtons Theorie wird daher als zwei Tetraeder des Modells visualisiert, aber er selbst hat diese Unterscheidung nie getroffen. Auf der anderen Seite wurden die Korrekturen $(0,c^{-1},0,0)$, $(0,0,h,0)$ und $(0,0,0,\kboltz)$ bekanntermaßen erst zu Beginn des 20.\ Jahrhundert eingeführt. Hier spiegelt das Modell die Wissensgeschichte wider.


\newpage \addsec{Klassische statistische Mechanik $(0,0,0,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:0001}

Das Modell des idealen Gases basiert auf einer großen Anzahl unterscheidbarer, klassischer, nichtrelativistischer Teilchen, die sich in einem geschlossenen Behälter befinden. Die Teilchen werden als punktförmig idealisiert. Es wird angenommen, dass ihre Bewegung den Gesetzen der reinen Newtonschen Mechanik folgt, und dass der Einfluß der Gravitation vernachlässigbar ist. Ihre freie Bewegung wird also nur durch vollkommen elastische Kollisionen untereinander oder mit den Wänden modifiziert.

Innerhalb der modernen quantitativen kinetischen Gastheorie wurde eine neue fundamentale Konstante mit der Bezeichnung $\kboltz$ definiert. Unter Verwendung dieser Konstante wird das ideale Gasgesetz wie folgt geschrieben

\begin{equation*}\label{eq:ideal-kb}
  p\, V=N\, \kboltz T\,,
\end{equation*}
%
wobei $p$, $V$ und $T$ den Druck, das Volumen und die Temperatur des Gases bezeichnen und $N$ für die (sehr große) Anzahl an Gasteilchen steht. Die Konstante $\kboltz$ dient somit als Proportionalitätsfaktor, der die durchschnittliche relative Wärmeenergie eines Gasteilchens mit der Temperatur in Beziehung setzt.

Als spezifische Naturkonstante wurde $\kboltz$ von Max Planck eingeführt. Sie wurde retrospektiv 1906 von Paul Ehrenfest \emph{Boltzmann-Konstante} genannt. Seit 2019 wird sie nicht mehr gemessen, sondern ist innerhalb des SI-Einheitensystems auf den \emph{exakten} Wert

\begin{equation*}
  \kboltz=\SI{1.380649d-23}{\joule \per \kelvin}
\end{equation*}
%
festgelegt.

Es scheint also, dass $\kboltz$ lediglich ein Umrechnungsfaktor zwischen Energie und Temperatur ist und damit wenig grundlegend sei. Im Rahmen des Hexadekachor-Modells jedoch wird $\kboltz$ zwingend erforderlich, um die tragende Rolle der statistischen Physik im Geflecht der physikalischen Theorien zu verstehen. Ausgehend vom innersten Tetraeder für die Newtonsche Mechanik gelangen wir so zum stachelförmigen Tetraeder $(0,0,0,\kboltz)$.


\subsection*{Kinetische Gastheorie}

In seiner \enquote{Hydrodynamica} (1738) schlug Daniel Bernoulli vor, dass die Temperatur eines idealen Gases über den Druck definiert sei. Obwohl Temperatur und kinetische Energie der Teilchen in Bernoullis Theorie zusammenkamen, glaubten die meisten Wissenschaftler des 18.\ Jahrhunderts an Newtons Theorie der Wärme als stofflicher Entität. Es dauerte weitere hundert Jahre, bis sich in der Physik kinetische Theorien durchsetzten, die von der keineswegs offensichtlichen empirischen Tatsache ausgingen, dass der (ideale) Gasdruck \emph{nicht} von der Molekülmasse des Gases abhängt. Dies führte zur ersten korrekten Formulierung eines allgemeinen Gasgesetzes durch Émile Clapeyron (1834):

\begin{equation*}\label{eq:ideal-nr}
  p\, V=n\, R\, T\,.
\end{equation*}
%
Dies ist die makroskopische Form der obigen Gleichung, wobei $p$, $V$ und $T$ wiederum dem Druck, dem Volumen und der Temperatur entsprechen, während $n$ die Stoffmenge und $R$ die universelle Gaskonstante ist. Diese allgemeine Gleichung warf die Frage auf, warum die Art des Gases für die Bestimmung des Gasdrucks irrelevant ist.

In den 1860er Jahren entwickelte James Clerk Maxwell die kinetische Beschreibung des idealen Gases weiter. Maxwell betrachtete seine Arbeit zunächst als eine Art statistische Fingerübung, da er (noch) nicht an das atomare Konzept der Materie glaubte. Später vertrat Ludwig Boltzmann nachdrücklich den atomaren Standpunkt und zeigte, dass die mittlere kinetische Energie eines Gasmoleküls tatsächlich nur von der Temperatur abhängt, womit er die moderne statistische Mechanik begründete.


\subsection*{Statistische Mechanik und Entropie}

Boltzmann gelang es, eine statistische Erklärung des zweiten Hauptsatzes der Thermodynamik zu finden. Dieser besagt, dass Wärme immer auf natürliche Weise von wärmeren zu kälteren makroskopischen Systemen fließt. Dabei interpretierte Boltzmann die Entropie $S$ von Clausius als \enquote{Grad der Unordnung}, der immer dann zunimmt, wenn keine Arbeit am System verrichtet wird. Max Planck fasste Boltzmanns Ergebnis später in der berühmten Formel 
%
\begin{equation*}\label{eq:ideal-nr}
  S=\kboltz\, \log W
\end{equation*}
%
prägnant zusammen, wobei $W$ die Anzahl der möglichen mikroskopischen Zustände ist, in denen sich ein bestimmtes makroskopisches System befinden kann, wie z.\,B.\ ein Gas bei festem Volumen, Druck und Temperatur.


\subsection*{Boltzmann-Statistik}

Betrachten wir ein System im thermischen Gleichgewicht bei einer Temperatur $T$. Die klassische Zustandssumme ist definiert als die Summe über alle möglichen mikroskopischen Zustände $j$ mit der Energie $E_j$

\begin{equation*}\label{eq:ideal-nr}
  Z=\sum_j \exp \left(-\frac{E_j}{\kboltz T}\right)\,,
\end{equation*}
%
wobei die Exponentialfunktion als \emph{Boltzmann-Faktor} bezeichnet wird. Die Wahrscheinlichkeit $p_k$, dass sich das System im $k$-ten Zustand befindet, ergibt sich dann aus

\begin{equation*}\label{eq:ideal-nr}
  p_k=\frac{1}{Z} \exp \left(-\frac{E_k}{\kboltz T}\right)\,.
\end{equation*}


\newpage
\addsec{Newtonsche Gravitationstheorie $(G,0,0,0)$ CK $\rightarrow$ ok}
\label{sec:1000}

Eine der größten Leistungen Newtons bestand in der Entdeckung seines allgemeinen Gesetzes der Gravitation. In moderner Formulierung besagt es, dass jeder Massenpunkt im Universum jeden anderen Massenpunkt \emph{augenblicklich} mit einer Kraft $F$ anzieht, die proportional zum Produkt ihrer Massen $m$ und $M$ und umgekehrt proportional zum Quadrat ihres Abstandes $r$ ist:

\begin{equation*}\label{eq:gravitation}
  F=G\,\frac{Mm}{r^2}\,.
\end{equation*}
%
Die zugehörige Proportionalitätskonstante $G$ wurde ihm zu Ehren viel später als Newtonsche Gravitationskonstante bezeichnet. Experimentell wurde $G$ erstmals, unbeabsichtigt und indirekt, 1798 von Henry Cavendish bestimmt, als er sein berühmtes Experiment zur Messung der Dichte der Erde durchführte. Der aktuelle Wert beträgt

\begin{equation*}
  G = \SI{6.67430(15)d-11}{\cubic\metre\per\kilogram\per\square\second}\,,
\end{equation*}
%
mit einer überraschend großen relativen Unsicherheit von \num{2,2 d-5}. Als solche ist sie die einzige der vier Fundamentalkonstanten des Hexadekachors $G$, $c^{-1}$, $h$ und $\kboltz$, die noch nicht auf einen exakten Wert festgelegt wurde, und deren Genauigkeit man mit verschiedenen Experimenten fortlaufend weiter zu erhöhen versucht.

Im Hexadekachor-Modell entspricht die Newtonsche Gravitation dem stachelförmigen Tetraeder $(G,0,0,0)$, der nach unten in Richtung Boden zeigt. In Kombination mit der klassischen Newtonschen Mechanik, d.\,h.\ dem innersten Tetraeder $(0,0,0,0)$, kann diese Theorie verwendet werden, um sowohl das Herunterfallen von Objekten auf der Erde als auch das \enquote*{Herumfallen} von Himmelskörpern wie Monden, Planeten, Meteoriten und Satelliten im Planetensystem mit erstaunlicher Genauigkeit zu beschreiben.


\subsection*{Newtons Gravitationstheorie}

Im dritten Buch \enquote{De mundi systemate} seiner \enquote{Philosophiæ Naturalis Principia Mathematica} führt Isaac Newton die Bewegung der Planeten und ihrer Satelliten auf die Wirkung einer universellen Gravitation zurück. Newton zufolge hält die resultierende Kraft alle Himmelskörper in ihrer Bahn. Sie verhält sich umgekehrt proportional zum Quadrat des Abstandes zwischen den Massenschwerpunkten zweier Objekte mit einer sphärisch symmetrischen Massenverteilung:

\begin{quote}
  Wenn die Materie zweier Kugeln, die sich gegenseitig anziehen, in den Bereichen, die sich in gleicher Entfernung vom Zentrum befinden, homogen ist, verhält sich das Gewicht jeder Kugel umgekehrt proportional zum Quadrat des Abstands zwischen ihren Zentren. (Newton 1687)
\end{quote}


\subsection*{Äquivalenzprinzip}

Ein integraler Bestandteil von Newtons Gravitationstheorie ist das schwache Äquivalenzprinzip. Dieses besagt, dass träge und schwere Masse eines jeden Körpers identisch sind. Bereits Galileo hatte experimentell nachgewiesen, dass zwei Objekte mit ungleicher Masse dennoch gleich schnell fallen, jedenfalls im Prinzip. Den notwendigen Beweis für diese Äquivalenz lieferte Newton experimentell mit einem speziellen Paar von Pendeln. Bis heute wird diese Äquivalenz mit äußerst komplexen Experimenten untersucht. Eine Abweichung zwischen schwerer und träger Masse hat sich dabei noch nie offenbart. Besonders eindrucksvoll demonstrierte die Mondmission Apollo 15, wie eine Feder und ein Hammer, die gleichzeitig aus derselben Höhe fallen gelassen wurden, gleichzeitig auf dem Boden auftrafen.


\subsection*{Fernwirkung und Einsteinsche Gravitation}

Für Newton besaß das Gravitationsgesetz ausschließlich relationalen Charakter; er führte die Gravitationskonstante $G$ nicht ein. Doch auch aus der Proportionalität der universellen Gravitation leitete er epochale Konsequenzen ab~-- zuvorderst, dass sich jeder Planet auf einer elliptischen Umlaufbahn um die Sonne bewegt, die sich in einem der Brennpunkte der Ellipse befindet. Newton formulierte damit eine dynamische und mathematische Herleitung für die bereits empirisch bekannte Bahn von Planeten: \enquote{whereas Kepler guessed right at the Ellipsis} (Newton an Halley 1686). Trotz der revolutionären Erfolge blieb für viele Zeitgenossen und Nachfolger Newtons bis hin zu Einstein der Charakter der instantanen Fernwirkung in seinem Gravitationsgesetz äußerst rätselhaft und fragwürdig. Letzterer löste das Rätsel nach der Entwicklung seiner Allgemeinen Relativitätstheorie, siehe $(G,c^{-1},0,0)$, die auch als Einsteinsche Gravitationstheorie bekannt ist.


\newpage \addsec{Statistische Mechanik und Newtonsche Gravitationstheorie $(G,0,0,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:1001}

Die Vereinigung $(G,0,0,\kboltz)$ von Newtonscher Gravitationtheorie $(G,0,0,0)$ und klassischer statistischer Mechanik $(0,0,0,\kboltz)$ verursacht weder technische noch konzeptionelle Probleme. Es gibt viele interessante Anwendungen, für die im Folgenden zwei Beispiele angeführt werden: Die barometrische Höhenformel für den atmosphärischen Druck in planetarischen Körpern wie unserer Erde und ein grobes Bild der Sternentstehung aus sich zusammenziehenden Gaswolken. Es sollte jedoch darauf hingewiesen werden, dass diese Vereinigung nur eine \enquote*{Summe ihrer Teile} darstellt, d.\,h.\ man erhält keine interessante, neue \enquote*{einheitliche Theorie}.


\subsection*{Barometrische Höhenformel}

Befindet sich ein ideales Gas aus Molekülen der Masse $m$ in einem Gravitationsfeld, das durch eine Masse $M$ erzeugt wird, entsteht ein barometrischer Gasdruck, der exponentiell im Verhältnis zur Gravitationsenergie der Gasmoleküle und ihrer thermischen Energie abnimmt. Um diese Beziehung herzuleiten, betrachtet man die Boltzmann-Verteilung der Moleküle in der Atmosphäre. Ein Molekül der Masse $m$, das sich in einer Höhe $H$ befindet, hat eine potentielle Energie $E=m g H$, wobei die Gravitationsbeschleunigung $g$ auf bzw.\ leicht über der Erdoberfläche durch die Newtonsche Gravitationskonstante $G$, sowie durch die Masse $M$ und den Radius $R$ der Erde ausgedrückt wird: $g=G M/R^2$. Daraus ergibt sich, dass die Wahrscheinlichkeitsverteilung der Moleküle proportional ist zu 

\begin{equation*}\label{eq:barometric_height}
  \exp\left(-\frac{m M}{R^2}\frac{G}{\kboltz} \frac{H}{T}\right).
\end{equation*}
%
Man beachte in dieser Formel das Verhältnis $G/\kboltz$ der beiden relevanten Fundamentalkonstanten.

Wir sollten diese von den situationsbezogenen (zufälligen) Konstanten $m, M, R^2$ unterscheiden. Die Variablen der Konstellation sind dagegen $H$ und $T$. Die genaue Wahrscheinlichkeitsverteilung ist dann gegeben durch

\begin{equation*}\label{eq:barometric_height_full_1}
  P(H)=\frac{m M}{R^2}\frac{G}{\kboltz T} \exp\left(-\frac{m M}{R^2}\frac{G}{\kboltz} \frac{H}{T}\right),
\end{equation*} 
%
was tatsächlich $\int_0^\infty dH P(H)=1$ erfüllt. Daraus folgt, dass das Verhältnis der Luftdrücke $p(H_2)$, $p(H_1)$ in den Höhen $H_2$, $H_1$ gegeben ist durch

\begin{equation*}\label{eq:barometric_height_full_2}
  \frac{p(H_2)}{p(H_1)}=\exp\left(-\frac{m M}{R^2}\frac{G}{\kboltz} \frac{H_2-H_1}{T}\right).
\end{equation*}


\subsection*{Sternentstehung}

Was ist die kritische Masse, bei der interstellare Gaswolken kollabieren und einen neuen Stern bilden? Die Jeans-Instabilität, benannt nach Sir James Jeans, beschreibt die Grenze, oberhalb derer der nach außen gerichtete interne Gasdruck nicht mehr stark genug ist, um die nach innen wirkende Gravitationskraft der Gesamtheit der Gasteilchen auszugleichen. Die Gaswolke kollabiert unter dem Einfluss der Schwerkraft, wodurch die erste Stufe der Bildung eines Protosterns erreicht wird.

Dieses Problem, das in der Astrophysik von entscheidender Bedeutung ist, kann näherungsweise durch die Anwendung der Grundprinzipien der Newtonschen Gravitation in Verbindung mit den Grundprinzipien der klassischen statistischen Mechanik behandelt werden. Natürlich sollte die Gesamtmasse nicht zu groß sein, und die Quanteneffekte der Kompression des Gases sollten vernachlässigt werden. Unter Verwendung einer Vielzahl weiterer Näherungen und vereinfachender Annahmen findet man dann die kritische \emph{Jeans-Masse}, oberhalb derer die potentielle Gravitationsenergie des Systems größer ist als die kinetische Energie seiner Komponenten, was zum Gravitationskollaps führt, der für die Geburt eines neuen Sterns erforderlich ist:

\begin{equation*}
  M_{\mathrm {Jeans}}=\alpha\,
    \left(\frac {\kboltz}{G}\right)^{\frac{3}{2}}\,
    \sqrt {{\frac {1}{\rho}}\, 
    \left({\frac {T}{m}}\right)^3}\,.
\end{equation*}
%
Hierbei ist $T$ die absolute Temperatur des Gases, $m$ die Masse seiner elementaren Bestandteile und $\rho$ ihre Dichte, während $\alpha$ eine numerische Konstante ist, die von den Details der Näherung abhängt (in der Regel findet man eine Zahl zwischen 1 und 10). Am relevantesten für das Hexadekachor-Modell ist, dass $M_{\mathrm {Jeans}}$ von den beiden Fundamentalkonstanten $\kboltz$ und $G$ abhängt. Beachten Sie, dass $c^{-1}$ und $h$ nicht auftauchen, da wir alle relativistischen bzw.\ alle Quanteneffekte außer Acht gelassen haben.


\newpage \addsec{Spezielle Relativitätstheorie und Elektrodynamik $(0,c^{-1},0,0)$ CK $\rightarrow$ ok}
\label{sec:0100}

In seinem \emph{annus mirabilis} 1905 veröffentlichte Albert Einstein vier berühmte Arbeiten. Im dritten Artikel \enquote{Zur Elektrodynamik bewegter Körper} stellte er die spezielle Relativitätstheorie vor. Diese basiert auf zwei Postulaten: der Invarianz des Inertialsystems~-- die Gesetze der Physik sind in allen Inertialsystemen identisch~-- und der Konstanz der Vakuumlichtgeschwindigkeit $c$, d.\,h., dass diese unabhängig vom Bewegungszustand der Lichtquelle ist. Die These, dass $c$ eine fundamentale Konstante für alle möglichen Beobachter sei, also unabhängig von deren eigenem Bewegungszustand, schien zumindest auf den ersten Blick widersprüchlich und erforderte ein radikales Hinterfragen der Annahmen von Galileo und Newton über die Struktur von Raum und Zeit. Als Ergebnis seiner Analyse erhielt Einstein die vielleicht berühmteste Formel der modernen Physik

\begin{equation*}\label{emc2}
  E=m\,c^2\,,
\end{equation*}
%
die die Äquivalenz von Masse ($m$) und Energie ($E$) ausdrückt.

Der erste Beweis für die Endlichkeit der Lichtgeschwindigkeit wurde 1676 von Ole Rømer erbracht, und die erste grobe Schätzung erfolgte zwei Jahre später durch Christiaan Huygens. Seitdem wurde die Genauigkeit ständig verbessert, bis die Lichtgeschwindigkeit schließlich 1983 festgelegt wurde auf den \emph{exakten} Wert 

\begin{equation*}
  c=\SI{299 792 458}{\meter \per \second}\,.
\end{equation*}

Im Hexadekachor-Modell stellt sich der relevante Parameter tatsächlich als die inverse Lichtgeschwindigkeit $c^{-1}$ heraus, da der nicht-relativistische Grenzwert durch die Annahme einer unendlichen Lichtgeschwindigkeit erhalten wird. Dementsprechend erscheint die spezielle Relativitätstheorie als stachelförmiger Tetraeder $(0, c^{-1}, 0, 0)$. Von zentraler Bedeutung im Hexadekachormodell ist, dass dieser Tetraeder auch die Maxwellsche Theorie des klassischen Elektromagnetismus umfasst. 


\subsection*{Elektromagnetismus}

Auch wenn mit der speziellen Relativitätstheorie die Lichtgeschwindigkeit als neue fundamentale Konstante der Physik verankert wurde, reichen ihre Anfänge bis ins 19.\ Jahrhundert zurück. 1873 vereinheitlichte James Clerk Maxwell nicht nur Elektrizität und Magnetismus, sondern identifizierte zudem Licht als ein elektromagnetisches Phänomen. Da der Elektromagnetismus als physikalisches Phänomen auf der Grundlage eines alles durchdringenden Äthers erklärt wurde, lag es nahe, dessen Existenz experimentell zu belegen. Die berühmten Interferometer-Experimente aus den 1880er Jahren von Edward Morley und Albert Michelson zeigten allerdings, dass die Ausbreitungsgeschwindigkeit von Licht in keiner Weise von der Bewegung der Erde durch einen Äther abhängt, und lieferten damit die experimentelle Grundlage für Einsteins Postulate. Es begann die komplexe Geschichte der Lorentz-Transformationen, die das exakte Transformationsverhalten der Maxwell-Gleichungen beschreiben.


\subsection*{Lorentz-Transformationen}

Die beiden Postulate der speziellen Relativitätstheorie sind nur dann miteinander vereinbar, wenn man die Lorentz-Transformationen verwendet, um zwischen den Koordinaten zweier Inertialsysteme zu wechseln, die sich mit konstanter Geschwindigkeit relativ zueinander bewegen. Die Galilei-Transformationen ergeben sich dann als Näherung für niedrige relative Geschwindigkeiten der Bezugssysteme. Darüber hinaus spiegeln sich die Konsequenzen für Bezugssysteme, die sich mit Geschwindigkeiten nahe der  Lichtgeschwindigkeit relativ zueinander bewegen, in zunächst rätselhaft erscheinenden Effekten wie der Zeitdilatation oder der Längenkontraktion wider.


\subsection*{Minkowski-Raumzeit}

1907 zog Hermann Minkowski die mathematischen und konzeptionellen Konsequenzen aus Einsteins Ideen und schlug auf revolutionäre Weise vor, den dreidimensionalen euklidischen Raum durch eine vierdimensionale Raumzeit zu ersetzen. Diese mathematisch präzise Verschmelzung von Raum und Zeit brachte die Physik auch in Kontakt zu philosophischen Konzepte wie der kosmologischen Ideen des Inka-Pacha oder der taoistischen Sichtweise von Raum und Zeit als \enquote*{komplexes kosmisches Netz}. Dies hinderte den Berliner Senat leider dennoch nicht daran, den 1994 verliehenen Ehrengrabstatus des Urnengrabs Minkowskis auf dem Berliner Friedhof Heerstraße 2014 wieder zu entziehen. 


In Anlehnung an Minkowskis Formalismus können wir den klassischen dreidimensionalen Impuls $\mathbf{p}$ zu einem Vierervektor verallgemeinern, dessen Zeitkomponente die Energie $E$ angibt. In der Minkowski-Raumzeit wird der Energie-Impuls-Vierervektor als $P = (E/c, \mathbf{p})$ ausgedrückt, mit der nicht-euklidischen quadrierten Norm $P^2=(E/c)^2 - \mathbf{p}^2 = m^2c^2$. Für den Spezialfall eines ruhenden Objekts, $\mathbf{p} = 0$, ergibt sich dann tatsächlich $E=m c^2$.

Wenn man andererseits $m=0$ setzt, erhält man die Beziehung zwischen der Energie und dem Impuls masseloser Teilchen wie Photonen, den Quanten von Licht und Strahlung:

\begin{equation*}\label{emc2}
  E=c\, |\mathbf{p}|\,.
\end{equation*}
%
Trotzdem weiß die spezielle Relativitätstheorie $(0,c^{-1},0,0)$ scheinbar nichts über die Theorie der Quantenmechanik $(0,0,h,0)$, die einem völlig anderen stachelförmigen Tetraeder des Hexadekachors entspricht!


\newpage \addsec{Spezielle Relativitätstheorie und statistische Mechanik $(0,c^{-1},0,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:0101}

Wie ändert sich die Temperatur bei Lorentz-Transformationen? Oder, direkter gefragt: Welche Temperatur hat ein bewegter Körper? Eine verallgemeinerte Theorie der Thermodynamik, die mit den Prinzipien der speziellen Relativitätstheorie korrekt vereint ist, d.\,h.\ eine Theorie der relativistischen Thermodynamik, ist seit ihrem ersten Vorschlag durch Max Planck und seinen Doktoranden Kurd von Mosengeil und unabhängig davon durch Albert Einstein im Jahr 1907 Gegenstand hitziger Debatten. Im Hexadekachor-Modell würde diese Theorie dem Tetraeder entsprechen, das mit $(0,c^{-1},0,\kboltz)$ bezeichnet ist.


\subsection*{Temperaturverwirrungen}

Welche thermodynamischen Größen bleiben bei Lorentz-Transformationen unverändert? Im Falle der Entropie, unter Berufung auf deren statistische Definition ~-- also die Anzahl der möglichen mikroskopischen Zustände eines gegebenen makroskopischen Systems~-- sind sich alle Autoren über ihre relativistische Invarianz einig. Bei Temperatur, Druck und den damit verbundenen thermodynamischen Potenzialen wurde jedoch noch kein vollständiger Konsens erzielt. Laut von Mosengeils posthum veröffentlichter Dissertation (er starb tragischerweise im Alter von 22 Jahren bei einer Wanderung in den Alpen) gilt:

\begin{quote}
  Zwei Körper, die der ruhende Beobachter als gleich heiß bezeichnet, [können] einem bewegten Beobachter verschieden heiß erscheinen [\dots], nämlich dann, wenn die Körper verschiedene Geschwindigkeit haben. Am höchsten wird die Temperatur eines Körpers immer dem Beobachter erscheinen, der relativ zu ihm ruht. (Von Mosengeil 1907)
\end{quote}
%
Seiner Meinung nach transformiert sich die Temperatur wie die Strahlungsintensität eines sich bewegenden schwarzen Körpers gemäß

\begin{equation*}\label{reltemplower}
  T = T_0\,\sqrt{1-\frac{v^2}{c^2}}\,,
\end{equation*}
%
wobei $T$ und $T_0$ die Temperaturen im bewegten bzw.\ ruhenden Bezugssystem sind und $v$ die relative Geschwindigkeit im Bezugssystem bezeichnet. Dieses Transformationsgesetz wird zwar in einem Großteil der neueren Literatur akzeptiert, aber eben nicht durchgängig.

Und in der Tat ist folgender Aspekt verwirrend: Wenn, wie oft behauptet wird, die Temperatur einfach eine Form von Energie wäre, die mit $\kboltz$ als konstantem Umrechnungsfaktor verknüpft ist, dann scheint das obige Transformationsgesetz falsch zu sein. In diesem Sinne machte Heinrich Ott 1963 (ebenfalls posthum) einen alternativen Vorschlag, der mit dem Verhalten von Energie unter Lorentz-Transformationen übereinstimmt. Er behauptete nämlich, dass bei der Ableitung des Lorentz-Transformationsgesetzes für Wärme und Temperatur ein grundlegender Fehler unterlaufen sei. Seiner Meinung nach sollte sich die Temperatur stattdessen wie folgt transformieren:

\begin{equation*}\label{reltemphigher}
  T = \frac{T_0}{\sqrt{1-\frac{v^2}{c^2}}}\,,
\end{equation*}
%
um eine Übereinstimmung mit dem zweiten Hauptsatz der Thermodynamik zu erreichen. Dieser widersprüchliche und alternative Vorschlag wird immer noch in einigen neueren Veröffentlichungen vertreten.

Um die Verwirrung zu vervollständigen, schlug Peter T.\ Landsberg 1966 eine neue Theorie vor, bei der die Temperatur eine Lorentz-Invariante ist:

\begin{equation*}\label{reltempequal}
  T = T_0\,.
\end{equation*}
%
Es besteht also kein Konsens über die Temperatur eines sich bewegenden Körpers. Dies scheint auf unterschiedliche Definitionen für Temperatur, Thermometer, Wärmeübertragung und Arbeit zurückzuführen zu sein. Aber vielleicht ist genau das die Lösung: Alle oben genannten Transformationsgesetze könnten anwendbar sein, abhängig von den ursprünglichen Annahmen~-- eine Schlussfolgerung, zu der Einstein 1952 gegen Ende seines Lebens gelangt zu sein scheint.


\subsection*{Relativistisches ideales Gas}

Es sollte darauf hingewiesen werden, dass Elektron-Positron-Paar-Plasmen, die unter experimentellen Bedingungen erzeugt wurden, Geschwindigkeitsverteilungen gemäß relativistischen Versionen der berühmten Maxwell-Boltzmann-Verteilung aufweisen. Diese wurden erstmals 1911 von Ferencz Jüttner ermittelt und werden als Maxwell-Jüttner-Verteilungen bezeichnet.

Zusammenfassend lässt sich sagen, wie im gerade betrachteten Beispiel, dass einige der Hexadekachor-Tetraeder noch keine sauber definierten vereinheitlichten Theorien bilden. Interessanterweise weisen sie damit vermutlich auf einige ungelöste Rätsel hin, die möglicherweise mit dem Fehlen einer endgültigen physikalischen Theorie, der schwer fassbaren \textit{theory of really everything} (TORE), also einer Theorie von wirklich Allem, zusammenhängen.


\newpage \addsec{Quantenmechanik $(0,0,h,0)$ CK $\rightarrow$ ok}
\label{sec:0010}

Zum Ende des 19.\ Jahrhunderts drangen die Experimente der Physik immer tiefer in den atomaren Bereich ein, mit sehr merkwürdigen, oft der Alltagslogik widersprechenden Ergebnissen. Um 1900 gelang es Max Planck, einige der experimentellen Merkwürdigkeiten durch Einführung einer neuen fundamentalen Konstante zu lösen, die wir heute als Plancksches Wirkungsquantum $h$ bezeichnen. 1905 entdeckte Einstein, um den photoelektrischen Effekt zu erklären, unfreiwillig die Lichtquanten, die später von Arthur Compton als Photonen bezeichnet wurden. Ihre Energie $E$ steht in Beziehung zur Frequenz $\nu$ des Lichts durch die Formel

\begin{equation*}
  E=h\,\nu\,.
\end{equation*}
%
Seit seiner Entdeckung wurde der Wert von $h$ immer genauer gemessen und schließlich 2019 auf den \emph{exakten} (rationalen) Wert  

\begin{equation*}
  h=\SI{6.62607015d-34}{\joule\second}
\end{equation*}
%
festgelegt. Während die Lichtgeschwindigkeit $c$ als experimenteller Wert bereits im 17.\ Jahrhundert bekannt war und durch Einsteins Relativitätstheorie in den Rang einer fundamentalen Naturkonstanten erhoben wurde, gehen die Geschichte und die Systematik des Hexadekachor-Modells auf das Jahr 1900 zurück: Planck bereitete unbeabsichtigterweise den Boden für eine neue Theorie $(0,0,h,0)$: die Quantenmechanik. Ihre tatsächliche mathematische und konzeptionell überzeugende Formulierung (\enquote{Von wegen!}, ruft Einstein aus seinem nicht existierenden Grab heraus) erfolgte ein Vierteljahrhundert später mit der Etablierung der Schrödingerschen Wellenmechanik und der Heisenbergschen Matrizenmechanik. Dadurch konnte die Wahrscheinlichkeit bestimmt werden, mit der sich ein Teilchen an einem bestimmten Ort befindet. Dementsprechend kann der genaue Ort nur durch eine Messung bestimmt werden. Letzteres führt jedoch zu einem Kollaps der Wellenfunktion. Dieses Phänomen ist für alle Messprozesse in der Quantenmechanik entscheidend. Dies führt zu einer Vielzahl weiterer kontraintuitiver Effekte, aber auch zu revolutionären technischen Entwicklungen.


\subsection*{Wirkungsprinzip}

Wie verhält sich die Quantenmechanik konzeptionell zur klassischen Mechanik, d.\,h.\ zur Theorie $(0,0,0,0)$? Die Newtonsche Mechanik scheint punktförmige Massen und ausgedehnte Körper wie Äpfel oder Planeten gleichermaßen gut zu beschreiben. Wie erstmals von Euler und Lagrange verstanden wurde, kann die klassische Physik durch die folgende Annahme hergeleitet werden: Die Natur wählt immer einen Pfad, der die \emph{Wirkung} minimiert. Allerdings ist eine Rechtfertigung dieses Wirkungsprinzips erforderlich. Hierfür mussten im 18.\ Jahrhundert metaphysische Prinzipien bemüht werden. Glücklicherweise oder unglücklicherweise versagt die Wirkungsminimierung und damit die Newtonsche Mechanik gänzlich auf den kleinsten Längenskalen, im Bereich der atomaren Dimensionen und darunter.


\subsection*{Hilbertraum}

Stattdessen geht die Quantenmechanik davon aus, dass ein gegebenes Teilchen alle möglichen Wege durchläuft, und weist dann jedem dieser Wege eine Wahrscheinlichkeit zu. Man betrachtet sogenannte Wahrscheinlichkeitsamplituden, deren Betragsquadrat die Wahrscheinlichkeitsdichten der jeweiligen Pfade angibt. Wird eine Messung durchgeführt, werden die oben genannten Amplituden abrupt und drastisch verändert. Abgesehen von möglichen neuen metaphysischen Bedenken, die mit diesem Ansatz verbunden sind, erlaubt die Quantenmechanik trotzdem weiter praktisch exakte Vorhersagen auf der Ebene makroskopischer Teilchensysteme.

Allerdings sind sämtliche überprüfbaren Vorhersagen der Quantenmechanik für Einzelteilchen prinzipiell statistisch. Zudem gilt, dass klassische Teilchen streng unterscheidbar sind. Quantenmechanische Teilchen gleicher Art dagegen können nicht voneinander unterschieden werden, während Teilchen unterschiedlicher Art im Hilbertraum verschränkt sein können. Dies erfordert eine andere Statistik als bei klassischen Teilchen.


\subsection*{Relation zur Relativitätstheorie}

Doch wie verhält sich die Quantenmechanik $(0,0,h,0)$ konzeptionell zur Relativitätstheorie? Die oben erwähnte Wahrscheinlichkeitsamplitude für ein freies Teilchen der Masse $m$, üblicherweise mit $\Psi$ bezeichnet, wird durch die (deterministische) Schrödinger-Gleichung (1926) bestimmt, also in diesem Fall

\begin{equation*}\label{eq:schroedinger}
  i\hbar\frac{\partial}{\partial t}\Psi=-\frac{\hbar^2}{2m}(\frac{\partial^2}{\partial x^2}+\frac{\partial^2}{\partial y^2}+\frac{\partial^2}{\partial z^2})\Psi
\end{equation*}
%
Man beachte die Asymmetrie von Raum und Zeit: Die zeitliche Ableitung ist erster Ordnung, die Ableitungen nach den Raumkoordinaten $x,y,z$ sind dagegen zweiter Ordnung. Kein Wunder, denn die Gleichung stammt aus der quantenmechanischen Verallgemeinerung der Newtonschen Mechanik und widerspricht somit der speziellen Relativitätstheorie. Und tatsächlich kommt die Lichtgeschwindigkeit $c$ in Schrödingers Gleichung nicht einmal vor.


\newpage \addsec{Statistische Quantenmechanik $(0,0,h,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:0011}

Die Verbindung von nicht-relativistischer Quantenmechanik $(0,0,h,0)$ und statistischer Mechanik $(0,0,0,\kboltz)$ führte zu einer perfekten \enquote*{Eheschließung}: der Quanten-Boltzmann-Statistik. Dabei entstand sogar ein wunderbarer Nachwuchs: Die Boltzmann-Statistik musste entsprechend angepasst werden, sobald die Statistik \emph{identische Teilchen} entweder bosonischer oder aber fermionischer Natur betraf.


\subsection*{Quanten-Boltzmann-Statistik}

Auf formaler Ebene lassen sich die Zustandssummen der klassischen statistischen Mechanik leicht an die Quantenmechanik anpassen, indem ein Dichteoperator $\hat\rho$ für ein kanonisches Teilchenensemble wie folgt eingeführt wird:

\begin{equation*}
  \hat \rho=\frac{1}{Z} \exp\left(-\frac{\hat H}{\kboltz T}\right)
    \qquad \textrm{mit} \qquad
    Z=\Tr \exp\left(-\frac{\hat H}{\kboltz T}\right)\,,
\end{equation*}
%
wobei $\hat H$ der Hamilton-Operator des Systems und $Z$ seine Zustandssumme ist, die als Spur über den Hilbert-Raum angegeben wird. Offenbar kann man $\hat\rho$ als eine normierte Operatorversion des klassischen Boltzmann-Faktors interpretieren, wobei $\kboltz$ explizit erscheint.

Der Dichteoperator ist offensichtlich auf Eins normiert: $\Tr \hat \rho=1$. In einer Basis von Energieeigenzuständen, bei denen $\hat H |\psi_n\rangle=E_n |\psi_n\rangle$, gilt

\begin{equation*}
  \hat \rho=\sum_n p_n |\psi_n\rangle \langle \psi_n|
    \qquad \textrm{mit} \qquad
    p_n=\frac{1}{Z} \exp\left(-\frac{E_n}{\kboltz T}\right)\,.
\end{equation*}
%
Hierbei ist $p_n$ die Wahrscheinlichkeit (und nicht die Wahrscheinlichkeitsamplitude!), dass der Zustand des Systems die Energie $E_n$ hat. In einem quantenmechanischen System ist die Anwesenheit des Planckschen Wirkungsquantums $h$ implizit; sie wird im Ausdruck für $\hat H$ erscheinen. Explizit taucht sie jedoch in der {von-Neumann-Gleichung} für die zeitliche Entwicklung des Dichteoperators auf:

\begin{equation*}\label{eq:vNeumann}
  i \hbar \frac{\partial \hat \rho}{\partial t}=\left[\hat H, \hat \rho\right]\,,
\end{equation*}
%
wobei die Klammern für einen Kommutator stehen. Dies kann als Verallgemeinerung der Schrödinger-Gleichung für reine Zustände zu einer Bewegungsgleichung für statistisch gemischte Zustände betrachtet werden. Man beachte, dass in dieser Interpretation $\hat \rho$ einen quantenstatistischen \emph{Zustand} im Gegensatz zu einem quantenmechanischen \emph{Operator} bezeichnet, für den ein Minuszeichen auf der rechten Seite dieses Zeitentwicklungsoperators erscheinen würde. Mit dem Dichteoperator $\hat \rho$ erhält man die \emph{von-Neumann-Entropie} $S$ durch eine weitere Spur über den Hilbertraum:

\begin{equation*}
  S=-\kboltz \Tr \left( \hat \rho \log \hat \rho \right).
\end{equation*}
%
Man achte wieder auf das explizite Auftreten von $\kboltz$ in dieser Gleichung.


\subsection*{Bose-Einstein-Statistik für Bosonen}

Für den Übergang von der klassischen Mechanik zur Quantenmechanik spielte die kinetische Gastheorie eine wichtige Rolle: Max Planck und Albert Einstein verwendeten die statistischen Methoden von James Clerk Maxwell und Ludwig Boltzmann, um eine Quantentheorie der Strahlung zu entwickeln. Die Quanten der Strahlung sind Photonen, die \emph{Bosonen} mit Spin Eins sind.

Zur Herleitung betrachtet man ein ideales bosonisches Quantengas, das an ein Wärmebad und ein Teilchenreservoir gekoppelt ist, und definiert die großkanonische Zustandssumme

\begin{equation*}
  Z_{\mathrm{G}}=\Tr \exp\left(-\frac{\hat H-\mu \bar N}{\kboltz T} \right)\,.
\end{equation*}
%
Die Spur wird über alle Energiezustände des Systems sowie über alle möglichen Teilchenzahlen $N$ genommen, wobei $\hat N$ der Teilchenzahloperator und $\mu$ das chemische Potential ist. Unter Annahme der grundlegenden Eigenschaft, dass die Energieeigenwerte von $\hat H$ durch eine beliebige Anzahl $n_\nu=0,1,2,3, \ldots$ von Bosonen erreicht werden können (die sogenannten Besetzungszahlen des $\nu$-ten Einteilchenenergiezustands $E_\nu$, so dass der Gesamtenergieeigenwert von $\hat H$ $E=\sum_\nu n_\nu E_\nu$ und der Gesamtzahleigenwert des Teilchenzahloperators von $\hat N$ $N=\sum_\nu n_\nu$ ist), erhält man den Erwartungswert

\begin{equation*}
  \langle n_\nu \rangle=
    \frac{1}{\exp\left(\frac{E_\nu-\mu}{\kboltz T} \right)-1}\,.
\end{equation*}
%
Dies ist die Bose-Einstein-Verteilung, die erstmals in Max Plancks Gesetz der Schwarzkörperstrahlung aus dem Jahr 1900 auftauchte. Plancks Konstante $h$ ist in $E_\nu$ verborgen.

Eine spannende Anwendung ist das Bose-Einstein-Kondensat, das 1924 vorhergesagt und 1995 erstmals gemessen wurde. Es handelt sich um einen Materiezustand, in dem ein Gas bosonischer (Quasi-)Teilchen (= Anregungen, die sich effektiv wie Quantenteilchen verhalten) auf Temperaturen unterhalb seiner kritischen Temperatur abgekühlt wird. Unter solchen Bedingungen \emph{kondensiert} eine Vielzahl von Teilchen in den niedrigsten Quantenzustand.


\subsection*{Fermi-Dirac-Statistik für Fermionen}

1926 schlugen Paul Dirac und Enrico Fermi die quantenmechanische Version eines idealen Gases vor, das aus nicht wechselwirkenden Fermionen wie Elektronen besteht und in der Festkörperphysik von großer Bedeutung ist. Diese gehorchen dem Pauli-Prinzip, was bedeutet, dass die oben betrachteten Besetzungszahlen nur noch $n_\nu=0,1$ sein dürfen. Man erhält dann für den Erwartungswert der Besetzungszahl $n_\nu$ des $\nu$-ten Einzelteilchenenergiezustands $E_\nu$ die Fermi-Dirac-Verteilung

\begin{equation*}
  \langle n_\nu \rangle=
    \frac{1}{\exp\left(\frac{E_\nu-\mu}{\kboltz T} \right)+1}\,.
\end{equation*}


\newpage \addsec{Allgemeine Relativitätstheorie $(G,c^{-1},0,0)$ CK $\rightarrow$ ok}
\label{sec:1100}

Um die Newtonsche Gravitation mit der speziellen Relativitätstheorie vereinen zu können, müssen beide Naturkonstanten $G$ und $c^{–1}$ berücksichtigt werden. Genau dies leistet Einsteins allgemeine Relativitätstheorie (1915). Sie verwandelt die statische vierdimensionale Minkowskische Raum-Zeit in ein dynamisches und gekrümmtes Objekt, dessen Form erst durch Einsteins Feldgleichungen eindeutig festgelegt ist.

In einer gekrümmten Raumzeit bewegen sich Objekte immer auf dem kürzestmöglichen Weg, der durch eine unbestimmte Metrik bestimmt wird. Ein historisch relevantes Beispiel ist die Umlaufbahn des Merkur um die Sonne. Einstein zufolge krümmt die Sonne die Raumzeit auf eine Weise, die zu einer nahezu elliptischen Umlaufbahn führt, bis auf eine kleine Perihelverschiebung. Im Gegensatz zu Newtons Ansatz ist der Grund für die gekrümmte Bahn des Merkur nicht irgendeine mysteriöse Gravitationskraft, die von der Sonne ausgeht, sondern einfache Ökonomie. Eine Analogie ist die Bahn eines Flugzeugs von Prag nach Seoul: Diese ist gekrümmt, aber mitnichten aufgrund einer hypothetischen Kraft am Nordpol.


\subsection*{Wirkungsprinzip}

Warum war sich Einstein so sicher, dass er eine neue Gravitationstheorie entwickeln musste, die die Konstanten $c^{−1}$ und $G$ in sich schlüssig vereinen würde? Nachdem Planck ihn nach Berlin geholt hatte, empfahl er ihm dringend, seine Zeit nicht zu vergeuden, sondern sich lieber auf die Entwicklung der Quantenmechanik zu konzentrieren. Einstein war sich allerdings sehr bewusst, dass Newton Gravitationstheorie und die spezielle Relativitätstheorie grundsätzlich unverträglich sind. So führt beispielsweise eine Veränderung des Abstands zweier Massen zu einer instantan gefühlten Veränderung der Schwerkraft zwischen den beiden. Im Gegensatz dazu erlaubt die spezielle Relativitätstheorie keine Signalübertragung mit einer größeren Geschwindigkeit als $c$.

Dieses grundlegende Problem, dass Newtons Gravitationsgesetz lediglich vom Ort, nicht aber von der Zeit abhängt, musste im Rahmen der allgemeinen Relativitätstheorie eliminiert werden. Der aus heutiger Sicht eleganteste Weg, letztere herzuleiten, basiert auf einem geeigneten Wirkungsprinzip: Man betrachtet alle möglichen Bewegungen eines Körpers in einen vorgegebenen Zeitintervall und bestimmt die tatsächliche Bahn nach dem Kriterium: \enquote{Der Pfad der tatsächlichen Fortpflanzung ist jener, zu dem die kleinste Wirkungsmenge gehört!} (Maupertuis 1744).


\subsection*{Einsteins Feldgleichungen}

Aus dem Wirkungsprinzip folgen bereits die berühmten Euler-Lagrange-Gleichungen:

\begin{equation*}\label{eq:euler-lagrange}
  \frac{d}{dt}\frac{\partial L}{\partial \dot{q}_i}-\frac{\partial L}{\partial q_i} = 0\,.
\end{equation*}
%
Hierbei ist $q_i(t)$ die Koordinate des $i$-ten Teilchens zum Zeitpunkt $t$ und $L$ der Lagrange-Operator, der durch die Differenz zwischen der kinetischen und der potentiellen Energie der Teilchen gegeben ist. Die Euler-Lagrange-Gleichungen liefern dann sofort die Newtonschen Bewegungsgleichungen.

Damit aber beruht diese Herleitung auf einer so nicht korrekten Asymmetrie von Raum und Zeit: Die unterschiedlichen Zustände eines Teilchens werden lediglich über die Zeit integriert. Um diese Idee auf die Einsteinsche Gravitationstheorie anwenden zu können, muss obige Raum-Zeitasymmetrie beseitigt werden. Man sucht nach einer geeigneten Wirkung $S$, die die verlangte Minimierung unter beliebigen Variationen der Metrik erlaubt:

\begin{equation*}\label{eq:einstein-hilbert-action}
  \frac{\delta S}{\delta g_{\mu\nu}} = 0\,.
\end{equation*}
%
Die tatsächliche Metrik errechnet sich dann wieder aus einem Analogon der Euler-Lagrange-Gleichungen. Die richtige Wahl für $S$ wird heute als Einstein-Hilbert-Wirkung bezeichnet und führt zu Einsteins Feldgleichungen:

\begin{equation*}\label{eq:field-equations}
  R_{\mu\nu}-\frac{1}{2}g_{\mu\nu}R = \frac{8\pi G}{c^4} T_{\mu\nu}\,.
\end{equation*}
%
Ihre mathematisch exakte Aussage ist: Materie ($T_{\mu\nu}$) krümmt ($R_{\mu\nu}$ und $R$) die Raumzeit ($g_{\mu\nu}$). Objekte bewegen sich in dieser Metrik auf gekrümmten Bahnen ohne jede Krafteinwirkung: Die Gravitation entspricht schlichtweg dem Effekt der Raum-Zeit-Krümmung.

Die Objekte, die sich im Kosmos bewegen, folgen in ihren Bahnen also nur einem Prinzip: Dem der kürzesten und schnellsten Verbindung in einer gekrümmten Raum-Zeit. Auf ihrem Weg krümmen sie mit ihrer Masse und Energie wiederum die Raum-Zeit in ihrer unmittelbaren Umgebung. Präzise formuliert: Alles bewegt sich auf einer vierdimensionalen Lorentzschen Mannigfaltigkeit, deren lokale Struktur wiederum durch diese Bewegung verändert wird. Die allgemeine Relativitätstheorie löst dadurch die Widersprüche zwischen der Newtonschen Gravitationstheorie und der speziellen Relativitätstheorie auf, und bringt damit die beiden Konstanten $G$ und $c^{-1}$ miteinander in Verbindung. Dies liefert die Theorie $(G,c^{-1},0,0)$ des Hexadekachors.


\newpage \addsec{Allgemeine Relativitätstheorie und statistische Mechanik $(G,c^{-1},0,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:1101}

Bereits die Vereinigung von spezieller Relativitätstheorie und statistischer Mechanik, repräsentiert durch das Tetraeder $(0,c^{-1},0,\kboltz)$, führt zu Problemen wie der korrekten Abhängigkeit der Temperatur vom Bezugssystem, die bis heute nicht auf allgemein akzeptierte Weise gelöst wurden. Wie zu erwarten, verbessert sich die Situation nicht, wenn auch die Schwerkraft einbezogen wird: Ein überzeugender Rahmen für die Vereinigung von Einsteins allgemeiner Relativitätstheorie und Boltzmanns statistischer Mechanik, der dem Tetraeder $(G,c^{-1},0,\kboltz)$ entspricht, muss erst noch entwickelt werden. Tatsächlich wurde dieses Problem schon früh von Max Planck und Albert Einstein erkannt, und es wurden einige Anstrengungen unternommen, um es zu lösen~-- jedoch ohne, dass man sich bisher vollständig einigen konnte.


\subsection*{Kosmologie und Temperatur}

Viele Probleme in der Kosmologie erfordern die Verwendung von Werkzeugen und Ideen aus sowohl der allgemeinen Relativitätstheorie als auch der Thermodynamik. Tatsächlich wurde der Urknall als sehr naheliegende Vorhersage der klassischen Relativitätstheorie entdeckt. Das physikalische Bild hinter der \enquote*{anfänglichen} Raum-Zeit-Singularität ist das eines extrem heißen, dichten Plasmas aus Strahlung und Elementarteilchen, das sich in und mit der Raumzeit als solcher extrem schnell ausdehnt. Die Entwicklung zum heutigen Zustand des Universums, etwa \num{13.8} Milliarden Jahre nach dem Urknall, wird für gewöhnlich als eine fortwährende Abkühlung aller Materie und Strahlung verstanden. Dementsprechend wird in der chronologischen Beschreibung der Entwicklung des Universums die Temperatur oftmals als Parameter verwendet. Für den Zeitpunkt des Urknalls wird angenommen, dass dort die Strahlungstemperatur bei etwa \qty{e32}{\kelvin} lag. Nach etwa \num{380000} Jahren hatte sich das Plasma auf \qty{3000}{\kelvin} abgekühlt, und Photonen begannen, sich frei durch den Raum zu bewegen.

Diese Strahlung ist heute noch als kosmische Mikrowellenhintergrundstrahlung (CMB) beobachtbar, aber ihre Temperatur beträgt jetzt nur noch \qty{2.7}{\kelvin}. Trotzdem ist nicht eindeutig klar, ob dieses kosmologische Bild wirklich in das Tetraeder $(G,c^{-1},0,\kboltz)$ aufgenommen werden sollte, im Gegensatz zu $(G,c^{-1},h,\kboltz)$, da die Strahlung letztlich auf Quanteneffekte zurückzuführen ist. Wir haben dies jedoch getan, weil die Kosmologie eine greifbare Beziehung zwischen Temperatur und allgemeiner Relativitätstheorie herstellt.


\newpage \addsec{Quantenfeldtheorie $(0,c^{-1},h,0)$ CK $\rightarrow$ ok}
\label{sec:0110}

Die Quantenmechanik wurde in den 1920er Jahren zunächst im nicht-relativistischen Grenzfall $c^{−1}=0$ formuliert, obwohl die Richtigkeit von Einsteins spezieller Relativitätstheorie bereits seit Jahren allgemein anerkannt war, mit der bemerkenswerten Ausnahme einiger antisemitischer Idioten. Dirac gelang es 1928, eine eigenartige relativistische Wellengleichung aufzustellen, die die Schrödinger-Gleichung auf höchst nicht-triviale Weise verallgemeinerte. Sie ist heute direkt vor Isaac Newtons Grab in der Westminster Abbey in der eleganten Form $i \gamma \cdot \partial\, \psi = m\, \psi$ eingraviert, wobei $c^{−1}$ und $\hbar=\frac{h}{2 \pi}$ auf eins gesetzt wurden. Wenn man sie wieder einsetzt, lautet die Dirac-Gleichung

\begin{equation*}
  i\, \hbar\,\, \gamma \cdot \partial\, \psi = m\,c\, \psi\,.
\end{equation*}
%
Insbesondere erklärte sie den Spin des Elektrons und sagte dessen Antiteilchen voraus. Ein Triumph war die experimentelle Bestätigung dieses Positrons durch Carl Anderson im Jahr 1932.

Allerdings führte die Interpretation der Dirac-Gleichung als Wellengleichung zu rätselhaften Widersprüchen. Zudem ergaben sich Probleme bei der Quantisierung der Elektrodynamik, bei der ebenfalls $c^{-1}$ eingeschaltet ist. Als Ausweg wurde die Quantenfeldtheorie (QFT) in den späten 1940er Jahre als eine relativistisch konsistente Vielteilchentheorie entwickelt. Diese führte etwa 20 Jahre später zum Standardmodell der Elementarteilchen, das seitdem alle Naturkräfte mit Ausnahme der Gravitation präzise beschreibt.


\subsection*{Standardmodell}

Die korrekte Quantisierung elektromagnetischer Felder~-- genauer gesagt der sogenannten abelschen Eichfelder~-- führt zur Quantenelektrodynamik (QED). Sie ist die quantitativ am genauesten überprüfte physikalische Theorie, die jemals aufgestellt wurde. Die Quanten dieser Felder sind nichts anderes als Einsteins Photonen, die ebenfalls einen Spin besitzen. Im Gegensatz zum Elektron ist deren Wert eins. Und auch die Kernkräfte beschreibt man sehr erfolgreich mit Quantenfeldtheorien, wobei weitere Spin-1-Teilchen zum Einsatz kommen: die acht Gluonen der starken Kernkräfte und die W$^+$, W$^-$- und Z-Bosonen der schwachen Kernkräfte. Um diese Teilchen und die ihnen entsprechenden Naturkräfte zu beschreiben, muss man die abelschen Eichfelder durch mathematisch deutlich komplexere nichtabelsche Felder ersetzen. Insgesamt ergibt sich daraus das aktuelle Standardmodell der Elementarteilchenphysik.

Seine nichtabelsche Eichsymmetriegruppe ist $\mathrm{SU}(3) \times \mathrm{SU}(2) \times \mathrm{U}(1)$, wobei der erste Faktor mit den starken Kernkräften (Quantenchromodynamik: QCD), der zweite mit den schwachen Kernkräften und der dritte mit der QED zusammenhängt. Ein offenes Rätsel ist, warum die Natur genau diese Eichgruppe wählt. Ein weiteres Rätsel sind die ungeklärten 19 freien Parameter des Standardmodells, das um weitere sieben freie Parameter für die schwer fassbaren, leicht massiven und chamäleonartigen Neutrinos erweitert wurde.

Besonders hervorzuheben ist das Higgs-Feld des Standardmodells, das bereits in den frühen 1960er Jahren vorhergesagt und 2012 nach langer Suche endlich mit dem Large Hadron Collider (LHC) am CERN in Genf entdeckt wurde. Es verleiht den meisten Elementarteilchen eine Masse, siehe das $m$ in der obigen Dirac-Gleichung im Fall der Spin $\frac12$ Teilchen.


\subsection*{Das Graviton}

Bisher nicht gelungen ist es, die Gravitationstheorie in das Standardmodell zu integrieren. Das dazugehörige hypothetische Teilchen, das die Gravitationskraft trägt, wurde allerdings schon getauft. Es heißt Graviton und müsste, wenn es existiert, den Spin 2 haben. Seine Einbeziehung würde vom Tetraeder $(0, c^{-1},h,0)$ zum Tetraeder $(G,c^{-1},h,0)$ des Hexadekachors führen, d.\,h.\ zur Theorie von allem (TOE).


\subsection*{Quantenfeldtheorie 2.0}

Als Theorie scheint die QFT trotz der großen Anzahl an Lehrbüchern, die darüber geschrieben wurden, bisher nicht wohldefiniert zu sein. Für einen Mathematiker ist sie daher wenig befriedigend. Der wichtigste praktische Ansatz, mit diesem Problem umzugehen, sind ausgefeilte Störungsmethoden, die auf Feynman-Diagrammen basieren, oder eine numerische, aber nicht-störende Neuformulierung, die als Gitter-Eichtheorie bezeichnet wird. Es gibt spannende Versuche, völlig neue Ansätze für die QFT zu entwickeln. Zudem gibt es faszinierende aktuelle Anwendungen des Formalismus der QFT in der Gravitationswellenforschung in der klassischen $(G,c^{-1},0,0)$-Theorie (= allgemeine Relativitätstheorie).


\newpage \addsec{Quantenfeldtheorie und Temperatur $(0,c^{-1},h,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:0111}

Interessanterweise ist die Quantenfeldtheorie (QFT) strukturell bereits auf unvermeidliche Weise eine Vielteilchentheorie: Selbst wenn man sich auf die Streuung von nur wenigen externen Teilchen beschränkt, treten notwendigerweise unendlich viele weitere interne, \enquote*{virtuelle} Teilchen in Zwischenkanälen des Streuprozesses auf. Hierbei spielt jedoch der Temperaturbegriff keinerlei Rolle. Dementsprechend wird die Boltzmann-Konstante $\kboltz$ für die anfängliche Formulierung der QFT zunächst nicht benötigt.

Dies ändert sich, wenn man eine große Anzahl externer physikalischer Teilchen betrachtet, wie z.\,B.\ die Photonen, die dem Welle-Teilchen-Dualismus zufolge die Strahlung eines schwarzen Körpers im thermischen Gleichgewicht mit der Umgebung bilden. Tatsächlich beschreibt das Plancksche Strahlungsgesetz (1900) genau diese Situation. So lautet die spektrale Strahlungsdichte $B(\nu,T)$ eines Körpers für die Frequenz $\nu$ bei der absoluten Temperatur $T$

\begin{equation*}
  B(\nu,T)=\frac{2 h \nu^3}{c^2} \frac{1}{\exp\left(\frac{h \nu}{\kboltz T}\right)-1}\,.
\end{equation*}
%
Dies war die erste experimentell ermittelte und theoretisch hergeleitete Quantenformel der Menschheit und enthält $\kboltz$ zusammen mit $c^{-1}$ und $h$. Man beachte, dass die Quantenmechanik ohne $k_B$ und $c^{-1}$ erst etwa 25 Jahre später entwickelt wurde und die eigentliche Quantenfeldtheorie, also die Theorie ohne $\kboltz$ aber mit $c^{-1}$, etwa 50 Jahre später! Tatsächlich basierte Plancks Herleitung ausschließlich auf statistischen Überlegungen in Verbindung mit intuitiven Annahmen über die Quantennatur der Strahlung. Die saubere theoretische Rechtfertigung seiner Formel basiert jedoch letztendlich sowohl auf der Quantenelektrodynamik (QED), einer Quantenfeldtheorie, als auch auf der Bose-Einstein-Statistik.


\subsection*{Quantenfeldtheorie bei endlicher Temperatur}

Es gibt jedoch noch eine weitere und ganz andere Möglichkeit, Quantenfeldtheorie und statistische Mechanik \emph{formal} in Beziehung zu setzen, die unter dem Namen \enquote*{QFT bei endlicher Temperatur} bekannt ist. Sie basiert auf einer tiefen Analogie zwischen dem quantenmechanischen Phasenfaktor $\exp\left(i \hbar S\right)$ und dem Boltzmann-Faktor $\exp\left(-\frac{E}{\kboltz T}\right)$. Formal wird die Zeit dabei imaginär, wodurch sich die Raumzeit in einen vierdimensionalen Zylinder mit drei unendlich ausgedehnten räumlichen Richtungen und einer periodischen \enquote{zeitlichen} Richtung mit Ausdehnung
$\frac{1}{\kboltz T}$ verwandelt. Wir haben hier also keine Theorie, die auf den drei Konstanten $c^{-1}$, $h$ und $\kboltz$ aufbaut, sondern eine, bei der wir von $c^{-1}$ und $h$ ausgehen und anschließend $h$ durch $\kboltz$ ersetzen gemäß der Regel 

\begin{equation*}
  \frac{t}{\hbar} \rightarrow \frac{1}{i \kboltz T}\,.
\end{equation*}


\subsection*{Unruh-Effekt}

Die \emph{Unruh-Temperatur}, die unabhängig von Paul Davies (1975) und William Unruh (1976) entdeckt wurde, ist die effektive Temperatur $T_{\text{U}}$, die von einem Detektor gemessen wird, der sich in einem Vakuumfeld mit einer gleichmäßigen Beschleunigung $a$ bewegt. Sie lautet

\begin{equation*}\label{eq:Unruh}
  T_{\text{U}}=\frac{\hbar}{2 \pi c\, \kboltz} a.
\end{equation*}
%
Das damit verbundene quantenfeldtheoretische Phänomen wird als \emph{Unruh-Effekt} bezeichnet. Dessen Existenz wird von einigen Physikern angezweifelt und konnte bislang experimentell nicht nachgewiesen werden, zumindest nicht auf allgemein anerkannte Weise. Bildlich gesprochen sagt er voraus, dass ein Thermometer mit einer Temperatur von null Grad in einem ansonsten leeren Raum plötzlich eine Temperatur anzeigt, wenn man damit herumwedelt. Der zugrunde liegende Mechanismus ist die Vakuumpolarisation, die in verschiedenen Bezugssystemen betrachtet wird.


\newpage \addsec{Nichtrelativistische Quantengravitation $(G,0,h,0)$ CK $\rightarrow$ ok}
\label{sec:1010}

Das Hinzuschalten der Gravitationskonstante $G$ zur nicht-relativistischen Quantenmechanik gestaltet sich zunächst denkbar unproblematisch: Man löst einfach die Schrödinger-Gleichung für massive Teilchen, die sich im Newtonschen Gravitationspotential bewegen. Interessanter wird es, wenn man die Existenz einer noch zu etablierenden \emph{theory of really everything} (TORE), also einer \emph{Theorie von wirklich Allem}, 
annimmt und sich dann vorstellt, darin die inverse Lichtgeschwindigkeit $c^{−1}$ sorgfältig abzuschalten, indem man in allen Formeln dieser hypothetischen Theorie mathematisch den Grenzwert $c\rightarrow\infty$ bildet.

Führt dies wirklich nicht zu messbaren Effekten? Haben wir wirklich nur das eher langweilige quantenmechanische Szenario, das eingangs skizziert wurde? Die Antwort steht noch aus. Insofern stellt die nicht-relativistische Quantengravitation gegenwärtig keine eigene Theorie der Physik dar und wird auch nur relativ wenig beforscht. Es sollte aber nicht unerwähnt bleiben, dass so mancher unkonventionelle Ansatz zur Quantengravitation mit genau dieser Möglichkeit der Brechung der Lorentz-Symmetrie zwischen Raum und Zeit spielt.


\subsection*{Experimentelle Befunde}

Seit den 1960er Jahren wurden Experimente mit Quantenteilchen in klassischen Gravitationsfeldern, und insbesondere dem Feld der Erde, durchgeführt. 
Offensichtlich ist es wichtig, langsame Quantenteilchen zu verwenden, wofür sich Neutronen besonders anbieten. So führten beispielsweise Colella, Overhauser und Werner 1975 ein Experiment durch, dessen Resultat sowohl von der Newtons Gravitationstheorie als auch von der Quantenmechanik abhing. Ein Strahl sehr langsamer Neutronen wurde zunächst aufgespalten und anschließend interferometrisch untersucht. Durch Rotation der Messvorrichtung um einen Winkel $\Phi$ konnten sie zeigen, dass eine quantenmechanische Phasenverschiebung der langsamen Neutronen durch deren Wechselwirkung mit dem Gravitationsfeld der Erde auftritt. Der experimentelle Befund stimmte jedoch bestens mit der theoretischen Vorhersage der Schrödinger-Gleichung mit Newtonschem Gravitationspotential überein, so dass sich (leider!) keine wirklich neuen Einsichten ergaben. Neuere Experimenten konnte sogar die Quantisierung schwacher Bindungszustände ultra-kalter, d.\,h.\ sehr langsamer Neutronen, im Gravitationsfeld der Erde nachweisen.


\subsection*{Weitere Hinweise}

Diese Art von Experiment ist möglicherweise relevant, um verschiedene Szenarien für die Theorie von Allem (TOE) zu unterstützen oder auszuschließen. Beispielsweise sagen einige Versionen der Superstringtheorie Abweichungen vom Newtonschen Gravitationspotenzial auf Längenskalen weit überhalb der Planck-Skala voraus. Allerdings konnten derartige Abweichungen bisher leider noch nicht gemessen werden.

Auch rein theoretisch inspiriert die durch den $c^{−1}\rightarrow 0$ Limes erzeugte Brechung der Lorentz-Symmetrie zwischen Raum und Zeit immer wieder die Forschung. So schlug beispielsweise Petr Hořava 2009 eine neue Gravitationstheorie vor, die die allgemeine Relativitätstheorie auf kleinen Längenskalen modifiziert. Dies hat bereits zu mehr als \num{2600} nachfolgenden Veröffentlichungen geführt. Dennoch ist der Status dieser sogenannten Hořava-Lifshitz Gravitationstheorie nach wie vor unklar.


\newpage \addsec{Nicht-relativistische Quantengravitation und Temperatur $(G,0,h,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:1011}

Zum jetzigen Zeitpunkt existiert noch keine allgemein anerkannte Theorie der Quantengravitation, auch wenn es experimentell nicht verifizierte und theoretisch unvollständige Kandidaten wie die Stringtheorie oder die Schleifenquantengravitation gibt. Diese Situation verbessert sich natürlich nicht durch die Einbeziehung der Temperatur, wenngleich es interessante Vermutungen über die Konsequenzen einer solchen allumfassenden \emph{theory of really everything} (TORE) gibt, also einer \emph{Theorie von wirklich Allem}, die alle vier Konstanten $(G, c^{-1}, h, \kboltz)$ mit einbezieht, wie etwa die Hawking-Strahlung oder die Bekenstein-Hawking-Entropie.

Man könnte vermuten, dass eine nicht-relativistische (d.\,h.\ $c^{-1}=0$) Version des gesuchten TORE-Modells, basierend auf den Konstanten $(G,0,h,\kboltz)$, leichter zu finden sein sollte. Dies scheint jedoch nicht der Fall zu sein, und es wurden im Grunde bisher keine wirklich brauchbaren Vorschläge gemacht. Allerdings gibt sehr interessante, aktuelle Experimente zu Quantenteilchen, Temperatur und Schwerkraft: Dabei handelt es sich um Studien zu ultra-kalten Neutronen (UCN), die auf frühe Ideen von Enrico Fermi aus der Mitte der 1930er Jahre zurückgehen.

UCN sind im Wesentlichen freie und extrem langsame (= kalte) Neutronen, die in einem geeigneten Behälter eingeschlossen werden. Dessen Wände reflektieren vollständig alle darauf aufprallenden Neutronen. Dadurch können UCN als eine Art verdünntes ideales Gas mit einer Temperatur von weniger als \qty{4}{\milli\kelvin} (Millikelvin) behandelt werden. Da die kinetische Energie der UCN sehr gering ist, spielt die Gravitationskraft eine entscheidende Rolle bei der Beschreibung des Systems. Diese Experimente werden als \enquote*{Neutronen in Flaschen} bezeichnet.

Es gibt ein interessantes Rätsel im Zusammenhang mit der Lebensdauer freier Neutronen, wenn man die in den Flaschenexperimenten gemessenen Werte mit denjenigen vergleicht, die in Neutronenstrahlexperimenten gemessen wurden. Die Lebensdauer, die aktuell (2024) von der Particle Data Group angegeben wird, beträgt \qty{878.4(0.5)}{\second} (ungefähr 15 Minuten). Dies liegt nahe an den Werten, die durch die Flaschenexperimente ermittelt wurden, aber etwa \qty{9}{\second} von den typischen Werten entfernt, die durch die Strahlenexperimente ermittelt wurden. Es gibt eine intensive Debatte darüber, wie man ein besseres experimentelles, aber vor allem theoretisches Verständnis dieser Diskrepanz erlangen kann.


\newpage \addsec{TOE: Theorie von Allem $(G,c^{-1},h,0)$ CK $\rightarrow$ ok}
\label{sec:1110}

Der bisher unentdeckte heilige Gral der modernen Physik. Diese Theorie würde die drei fundamentalen Konstanten $G,c^{−1}$ und $h$ umfassen, unter Ausschluss Plancks vierter Wahl $\kboltz$, der Boltzmann-Konstante. Es gibt eine ganze Reihe ambitionierter Anwärter auf den Thron, allen voran die Stringtheorie und die Schleifenquantengravitation. Allerdings ließ sich bisher für keine dieser beiden Theorien überprüfen, ob sie korrekt ist~-- vermutlich sind am Ende sogar beide falsch, zumindest aber unvollständig. Durch die Kombination der drei fundamentalen Naturkonstanten $G$, $c^{-1}$ und $h$ können wir die natürlichen Einheiten für Raum, Zeit und Masse ableiten, die sogenannten Planck-Einheiten~-- wobei die Temperatur außer Acht gelassen wird. Dabei stellt sich heraus, dass die Quantengravitation in Größenordnungen relevant wird, die diesen Planck-Einheiten entsprechen.

Leider sind diese Skalen extrem weit von allem entfernt, was wir experimentell direkt erschließen können. So liegt die Planck-Länge bei etwa $10^{−35}$ Metern, das ist eine geradezu unvorstellbare $10^{20}$-mal kleinere Skala als die Größe des Protons.


\subsection*{Eine Weltformel?}

Was ist nun die Theorie $(G,c^{-1},h,0)$ des Hexadekachors der Physik? Sie sollte mathematisch konsistent und falsifizierbar sein, also experimentell überprüfbare Vorhersagen liefern können. Im Deutschen wird diese gesuchte Theorie oft verkürzt als \enquote{Weltformel} (\enquote{world formula}) bezeichnet, obwohl es natürlich \textit{a priori} unklar ist, ob sie im Falle ihrer Existenz auf einer einzigen Formel basiert. Im Englischen heißt sie dagegen ganz unbescheiden \enquote{theory of everything} (TOE), übersetzt also \enquote{Theorie von Allem}.

Die Theorie von Allem sollte insbesondere Quantenfeldtheorie und allgemeine Relativitätstheorie vereinen. Die Gravitation soll also \enquote*{quantisiert} und die Konstante $h$ mit der Gravitation in Verbindung gebracht werden. Ebenso sollte $G$ mit der Quantenfeldtheorie verbunden werden. \enquote*{Quantisiert} man jedoch die allgemeine Relativitätstheorie in Analogie zur Elektrodynamik auf naive Weise, z.\,B.\ indem man über allen möglichen Metriken summiert, führt dies zu einer Katastrophe: Eine unendliche Vielzahl von unbezähmbaren \enquote*{Divergenzen} zerstört die Voraussagekraft der Theorie.

Die beiden prominentesten aller aktuell verfolgten Ansätze, die diese Schwierigkeiten zu umgehen suchen, sind die Superstringtheorie und die Schleifenquantengravitation. Beiden ist gemein, dass sie von einer experimentellen Überprüfbarkeit weit entfernt sind, und, damit zusammenhängend, dass es jeweils eine ungeheure Vielzahl unterschiedlicher Versionen gibt. Es handelt sich also eher um Theorieklassen als um eindeutig definierte Theorien.


\subsection*{Superstringtheorie}

Die Superstringtheorie hat ihre Wurzeln in der Quantenfeldtheorie. Deren Punktteilchen werden durch schwingende Saiten, die sogenannten Strings, ersetzt, um die verschiedenen Teilchen der Natur als Schwingungszustände der Strings darstellen zu können. Durch diesen Ansatz verschwinden die genannten Divergenzen, und Teilchen- und Gravitationsphysik werden vereint. Ein hypothetisches \enquote{Graviton} tritt auf, alle Welten werden zehn- oder sogar elfdimensional, und unsere eigene Welt ist offenbar nur eine von mindestens $10^{500}$ vielen. Einige dieser Welten ähneln zumindest dem Standardmodell der Teilchenphysik, aber leider wurde noch keine exakte Übereinstimmung entdeckt. Ein spekulativer Ausweg ist die Idee des Multiversums, in dem viele dieser möglichen Welten in Paralleluniversen existieren, die, etwas deprimierend, nicht miteinander kommunizieren. In diesem Szenario wären die vielen noch ungeklärten Parameter des Standardmodells letztlich entweder zufällig gesetzt oder aber schlichtweg dadurch festgelegt, dass sie intelligentes Leben ermöglichen (anthropisches Prinzip). Nicht so schön.


\subsection*{Schleifenquantengravitation}

Die Schleifenquantengravitation hat ihre Wurzeln in der allgemeinen Relativitätstheorie. Letztere wird direkt quantisiert; dies führt zu einer Art \enquote*{Quantenschaum} auf der Planck-Skala im Bereich von $10^{-35}$ Metern. Es gibt viele sich teilweise widersprechende Zugänge, aber in bisher keinem eine natürliche Vereinigung mit den Quantenfeldtheorien des Standardmodells. Diese müssen im Grunde genommen \enquote*{von Hand} hinzugefügt werden. Auch nicht schön.


\subsection*{Offene Fragen \dots}

Benötigt man überhaupt eine Theorie, die Quantenfeldtheorie und allgemeine Relativitätstheorie vereint? In dieser Frage herrscht weitgehende, bejahende Einigkeit in der Physik. Es ist nämlich im Rahmen der aktuellen physikalischen Theorien offensichtlich, dass sowohl die Detailbeschreibung von Schwarzen Löchern als auch diejenige des Urknalls die Quantenphysik erfordern. Zudem ist es zumindest experimentell klar, dass etwas Grundlegende im aktuellen Theoriegebäude der Physik fehlt: Etwa 96 Prozent der Materie und Energie unseres Universums sind wohl noch unbekannt, oder, wie man sagt, \enquote*{dunkel}.


\newpage \addsec{TORE: Theorie von wirklich Allem $(G,c^{-1},h,\kboltz)$ CK $\rightarrow$ ok}
\label{sec:1111}

Das äußerste Tetraeder des Hexadekachors, das alle anderen 15 Tetraeder umfasst, sollte die ultimative 
\emph{theory of really everything} (TORE) sein, also die \emph{Theorie von wirklich Allem}.
Diese wäre die vereinheitlichte Theorie hinter allen vier Planckschen Naturkonstanten $(G,c^{-1},h,\kboltz)$, die dieser 1899 nutzte, um universelle Einheiten für Länge, Zeit, Masse und Temperatur zu definieren. Obwohl uns eine experimentell verifizierbare TORE eindeutig fehlt und obwohl es anderseits nicht einmal klar ist, dass sie existieren muss, gibt es mittlerweile eine ganze Reihe wichtiger Hinweise. Sie stehen alle im Zusammenhang mit Schwarzen Löchern, die eine fundierte Vorhersage von Einsteins allgemeiner Relativitätstheorie $(G, c^{–1}, 0, 0)$ sind. Deren tatsächliche physische Existenz wurde in den vergangenen Jahren zweifelsfrei experimentell nachgewiesen.


\subsection*{Schwarze Löcher}

Karl Schwarzschild sagte 1916 Schwarze Löcher vorher, nachdem er Einsteins Feldgleichungen der allgemeinen Relativitätstheorie $(G,c^{-1},0,0)$ in einem Schützengraben einer eingehenden Analyse unterzogen hatte. Nach ihm ist ein Schwarzes Loch ein Raum-Zeit-Gebiet mit einem so starken Gravitationsfeld, dass alles einschließlich des Lichts daran gehindert wird, daraus zu entkommen.

Stephen Hawking entdeckte jedoch 1974, dass die Einbeziehung der Quantenfeldtheorie $(0,c^{-1},h,0)$ zu etwas anderen Vorhersagen führt. Hawking konnte theoretisch beweisen, dass Schwarze Löcher eine absolute Temperatur $T_\mathrm{H}$ haben, die heute als \emph{Hawking-Temperatur} bezeichnet wird, und folglich Wärmestrahlung emittieren. Dementsprechend sind sie nicht ganz so \enquote*{schwarz}, wie sie zunächst scheinen, und ihre korrekte theoretische Beschreibung erfordert die Thermodynamik $(0,0,0,\kboltz)$!


\subsection*{Hawking-Strahlung}

Die Vakuumpolarisation führt zu einem \enquote*{glühenden Horizont}. Mit dieser Teilchenstrahlung aus dem Horizont heraus ist eine Temperatur des Schwarzen Lochs verbunden, die Hawking-Temperatur $T_{\text{H}}$: Schwarze Löcher kann man im Prinzip \emph{sehen}. Die Existenz der Hawking-Strahlung wurde jedoch noch nicht experimentell bestätigt. Die Einführung einer Temperatur bedeutet, dass es in der Nähe eines Schwarzen Lochs ein Vielteilchensystem gibt. 
%Können wir eine Formel für diese Temperatur finden und sie durch Dimensionsanalyse rigoros definieren?


\subsection*{Hawking-Temperatur}

Die berühmte Formel von Hawking für die Temperatur $T_{\text{H}}$ eines Schwarzen Lochs als Funktion seiner Masse $M$ verknüpft nun alle vier fundamentalen Konstanten von Planck:

\begin{equation*}\label{eq:HawkingT}
  T_{\text{H}} = \frac{1}{8 \pi}\, \frac{\hslash\, c^3}{\kboltz\, G}\, \frac{1}{M}
\end{equation*}
%
Wenn $\hslash=\frac{h}{2\pi}\rightarrow0$, dann $T_\text{H}\rightarrow0$, und somit gäbe es keine Hawking-Strahlung: Es handelt sich also wirklich um einen Quanteneffekt!

Aufgrund der Vakuumpolarisation ist die Quantenfeldtheorie von Natur aus eine Vielteilchentheorie. In Horizontnähe eines Schwarzen Lochs können virtuelle Teilchen real werden. Die entscheidende Einsicht ist daher: Die Zusammenführung von Quantenfeldtheorie und Gravitation zwingt uns, auch die Temperatur und die Thermodynamik zu berücksichtigen.


\subsection*{Bekenstein-Hawking-Entropie}

Bekenstein postulierte: Da Schwarze Löcher aufgrund der Hawking-Strahlung intrinsisch thermodynamische Objekte sind, müssen sie eine Entropie besitzen. Und diese nimmt gemäß dem zweiten Hauptsatz der Thermodynamik immer zu.

Daraus ergibt sich ein zweites bahnbrechendes Ergebnis, das alle vier Fundamentalkonstanten miteinander verknüpft: die Entropie $S_{\text{BH}}$ eines Schwarzen Lochs nach Bekenstein und Hawking. Sie besagt, dass $S_{\text{BH}}$ proportional zur Fläche $A$ des zweidimensionalen Horizonts des Schwarzen Lochs ist:

\begin{equation*}
  S_{\text{BH}} = \frac{\kboltz\, c^3}{\hslash\, G}\, \frac{A}{4}
\end{equation*}
%
Dies ist seltsam und mysteriös und seit seiner Entdeckung Gegenstand hitziger Debatten: In \enquote*{üblichen} thermodynamischen Systemen ist die Entropie immer extensiv, d.\,h.\ proportional zu einem dreidimensionalen Volumen. Ist dies der schlagende Beweis, der auf die Existenz der TORE, also der endgültigen \emph{Theorie von wirklich Allem}, hinweist?


\end{document}